# Constructions and Idioms {#constructions-and-idioms}

```{r include=F, purl=F}
knitr::opts_chunk$set(message = F, warning = F)
library(showtext)
font_add("Arial Unicode MS", "Arial Unicode.ttf")
## Automatically use showtext to render plots
## If loading showtext throws errors, install: https://www.xquartz.org/
showtext_auto(enable = TRUE)
```


## Loading packages

```{r}
library(tidyverse)
library(tidytext)
library(quanteda)
library(stringr)
library(jiebaR)
library(readtext)
```

## Collostruction

In this tutorial, I would like to talk about the relationship between a construction and words. Words may co-occur to form **collocation** patterns. When words co-occur with a particular morphosyntactic pattern, they would form **collostruction** patterns.

Here I would like to introduce a widely-applied method for research on the meanings of constructional schemas---**Collostructional Aanalysis** [@stefanowitsch2003]. This is the major framework in corpus linguistics for the study of the relationship between words and constructions.

The idea behind collostructional analysis is simple: the meaning of a morphosyntactic construction can be determined very often by its co-occurring words.

In particular, words that are strongly associated (i.e., co-occurring) with the construction are referred to as **collexemes** of the construction.

**Collostructional Analysis** is an umbrella term, which covers several sub-analyses for constructional semantics:

-   **collexeme** analysis (cf. @stefanowitsch2003)
-   **co-varying collexeme** analysis (cf. @stefanowitsch2005)
-   **distinctive collexeme** analysis (cf. @gries2004)

This tutorial will focus on the first one, collexeme analysis, whose principles can be extended to the other analyses.

Also, I will demonstrate how we can conduct a collexeme analysis by using the R script written by Stefan Gries ([Collostructional Analysis](http://www.stgries.info/teaching/groningen/index.html)).

## Corpus

In this tutorial, I will use the Apple News Corpus as a toy corpus. (It is available in: `demo_data/applenews10000.tar.gz`.)

And in this demonstration, I would like to look at a particular morphosyntactic frame in Chinese, `X + 起來`. Our goal is simple: in order to find out the semantics of this constructional schema, it would be very informative if we can find out which words tend to strongly occupy this `X` slot of the constructional schema.

That is, we are interested in the collexemes of the construction `X + 起來` and their degree of semantic coherence.

So our first step is to load the text collections of Apple News into R and create a `corpus` object.

```{r purl=T}
## Text normalization function
## Define a function
normalize_document <- function(texts) {
  texts %>%
    str_replace_all("\\p{C}", " ") %>%  ## remove control chars
    str_replace_all("\\s+", "\n")  ## replace whitespaces with linebreak
}

## Load Data
apple_df <-
  readtext("demo_data/applenews10000.tar.gz", encoding = "UTF-8") %>% ## loading data
  filter(!str_detect(text, "^\\s*$")) %>% ## removing empty docs
  mutate(doc_id = row_number()) ## create index
```

```{r}
## Example usage of `normalize_document()`
## before cleaning
apple_df$text[1]
## after cleaning
normalize_document(apple_df$text[1])
```

```{r}
## Apply cleaning to all docs
apple_df$text <- normalize_document(apple_df$text)
```

::: try
Raw texts usually include a lot of noise. For example, texts may include invisible **control characters**, **redundant white-spaces**, and **duplicate line breaks**. These redundant characters may have an impact on the word segmentation performance. It is often suggested to clean up the raw texts before tokenization.

In the above example, I created a simple function, `normalize_text()`. You can always create a more sophisticated one that is designed for your own corpus data.
:::

## Word Segmentation

We use the self-defined word tokenization method based on `jiebar`. There are three important steps:

1.  We first initialize a `jiebar` language model using `worker()`;
2.  We then define a function `word_seg_text()`, which enriches the raw texts with word boundaries and parts-of-speech tags;
3.  Then we apply the function `word_seg_text()` to the original texts and create a new column to our text-based data frame, a column with the enriched versions of the original texts.

This new column will serve as the basis for later construction extraction.

```{r purl=T}
# Initialize jiebar
segmenter <- worker(bylines = F,
                    symbol = T)

# Define function
word_seg_text <- function(text, jiebar) {
  segment(as.character(text), jiebar) %>% # word tokenize
    str_c(collapse = " ") ## concatenate word tokens into long strings
}

# Apply the function
apple_df <- apple_df %>%
  mutate(text_tag = map_chr(text, ~ word_seg_text(.x, segmenter)))
```

```{r echo=F, purl=F}
apple_df %>%
  head(100) %>%
  mutate(text = str_sub(text, 0,100) %>% sprintf("%s...",.),
         text_tag = str_sub(text_tag, 0,100) %>% sprintf("%s...",.))
```

::: info
Our self-defined function `word_seg_text()` is a simple function to convert a Chinese raw text into am enriched version with word boundaries and POS tags.

```{r}
word_seg_text(apple_df$text[1], segmenter)
```
:::

::: alert
Please note that in this example, we did not include the parts-of-speech tagging in order to keep this example as simple as possible. However, in real studies, we often need to rely on the POS tags if we want to improve the quality of the pattern retrieval.

Therefore, for your own project, probably you need to revise the `word_seg_text()` and also the `jiebar` language model (`worker()`) if you would like to include the POS tag annotation in the data processing.
:::

## Extract Constructions

With the word boundary information, we can now extract our target patterns from the corpus using regular expressions with `unnest_tokens()`.

```{r purl=T}
# Define regex
pattern_qilai <- "[^\\s]+\\s起來\\b"

# Extract patterns
apple_qilai <- apple_df %>%
  select(-text) %>% ## dont need original texts
  unnest_tokens(
    output = construction, ## name for new unit
    input = text_tag, ## name of old unit
    token = function(x) ## unesting function
      str_extract_all(x, pattern = pattern_qilai)
  )

# Print
apple_qilai
```

## Distributional Information Needed for CA

To perform the **collexeme analysis**, which is essentially a statistical analysis of the association between words and a specific construction, we need to collect necessary distributional information of the words (`X`) and the construction (`X + 起來`).

In particular, to use Stefan Gries' R script of Collostructional Analysis, we need the following information:

1.  Joint Frequencies of the words and the construction
2.  Frequencies of Words in Corpus
3.  Corpus Size (total number of words in corpus)
4.  Construction Size (total number of the construction tokens in corpus)

::: info
Take the word `使用` for example. We need the following distributional information:

1.  Joint Frequencies: the frequency of `使用＋起來`
2.  The frequency of `使用` in Corpus
3.  Corpus Size (total number of words in corpus)
4.  Construction Size (total number of the construction tokens in corpus)
:::

### Word Frequency List

Let's attend to the second distributional information needed for the analysis : the frequencies of words/collexemes.

It is easy to get the word frequencies of the entire corpus.

With the tokenized texts, we first convert the **text-based** data frame into a **word-based** one; then we create the word frequency list via simple data manipulation tricks.

```{r purl=T}
## create word freq list
apple_word_freq <- apple_df %>%
  select(-text) %>% ## dont need original raw texts
  unnest_tokens( ## tokenization
    word,  ## new unit
    text_tag,  ## old unit
    token = function(x)  ## tokenization function
      str_split(x, "\\s+")
  ) %>%
  filter(nzchar(word)) %>% ## remove empty strings
  count(word, sort = T)

apple_word_freq %>%
  head(100)
```

::: alert
In the above example, when we convert our data frame from a text-based to a word-based one, we didn't use any specific tokenization function in `unnest_tokens()` because we have already obtained the **enriched version** of the texts, i.e., texts where each word token is delimited by a white-space. Therefore, the `unnest_tokens()` here is a lot simpler: we simply tokenize the texts into word tokens based on the **known delimiter**, i.e., the white-spaces.
:::

### Joint Frequencies

Now let's attend to the first distributional information needed for the analysis: the joint frequencies of `X` and `X+起來` construction.

With all the **pattern-based** data frame, `apple_qilai`, this should be simple. Also, because we have created the word frequency list of the corpus, we can include the frequency of the collexeme in our table as well.

```{r purl=T}
## Joint frequency table
apple_qilai_freq <- apple_qilai %>%
  count(construction, sort = T) %>%  ## get joint frequencies
  tidyr::separate(col = "construction", ## restructure data frame
                  into = c("w1", "construction"),
                  sep = "\\s") %>%
  ## identify the freq of X in X_起來
  mutate(w1_freq = apple_word_freq$n[match(w1, apple_word_freq$word)])

apple_qilai_freq
```

### Input for `coll.analysis.r`

Now we have almost all distributional information needed for the Collostructional Analysis.

Let's see how we can use Stefan Gries' script, `coll.analysis.r`, to perform the collostructional analysis on our data set.

The script `coll.analysis.r` expects a particular input format.

The input file should be a `tsv` file, which includes a three-column table:

1.  Words
2.  Word frequency in the corpus
3.  Word joint frequency with the construction

```{r purl=T}
## prepare a tsv
## for coll analysis
apple_qilai_freq %>%
  select(w1, w1_freq, n) %>%
  write_tsv("qilai.tsv")
```

::: alert
In the later Stefan Gries' R script, it requires that the input be a tab-delimited file (`tsv`), not a comma-delimited file (`csv`).
:::

### Other Information

In addition to the input file, Stefan Gries' `coll.analysis.r` also requires a few general statistics for the computing of association measures.

We prepare necessary distributional information for the later collostructional analysis:

1.  Corpus size: The total number of words in the corpus
2.  Construction size: the total number of the construction tokens in the corpus

Later when we run Gries' script, we need to enter these numbers manually in the terminal.

```{r purl=T}
## corpus information
cat("Corpus Size: ", sum(apple_word_freq$n), "\n")
cat("Construction Size: ", sum(apple_qilai_freq$n), "\n")
```

------------------------------------------------------------------------

::: try
Sometimes you may need to keep important information printed in the R console in an external file for later use. There's a very useful function, `sink()`, which allows you to easily keep track of the outputs printed in the R console and save these outputs in an external text file.

```{r purl=T, results=F, collapse=T}
## save info in a text
sink("qilai_info.txt") ## start flushing outputs to the file not the terminal
cat("Corpus Size: ", sum(apple_word_freq$n), "\n")
cat("Construction Size: ", sum(apple_qilai_freq$n), "\n")
sink() ## end flushing
```

You should be able to find a newly created file, `qilai_info.txt`, in your working directory, where you can keep track of the progress reports of your requested information.

Therefore, `sink()` is a useful function that helps you direct necessray terminal outputs to an external file for later use.
:::


## Running Collostructional Analysis

::: warning
Stefan Gries' `coll.analysis.r` will initialize the analysis by first **removing all the objects** in your current R session. Please make sure that you have saved all necerssary information/objects in your current R session before you source the script.
:::

Finally we are ready to perform the collostructional analysis using Stefan Gries' [`coll.analysis.r`](http://stgries.faculty.linguistics.ucsb.edu/teaching/groningen/coll.analysis.r).

We can use `source()` to run the entire R script. The `coll.analysis.r` is available on Stefan Gries' website. We can either save the script onto our laptop and run it offline or source the online version ( [`coll.analysis.r`](http://stgries.faculty.linguistics.ucsb.edu/teaching/groningen/coll.analysis.r)) directly.

```{r eval=F, echo=T, purl=T}
######################################
##       WARNING!!!!!!!!!!!!!!!     ##
## The script re-starts a R session ##
######################################
source("https://www.stgries.info/teaching/groningen/coll.analysis.r")

```

------------------------------------------------------------------------

`coll.analysis.r` is an R script with interactive instructions.

When you run the analysis, you will be prompted with guided questions, to which you would need to fill in necessary information/answers in the R terminal.

For our current example, the answers to be entered for each prompt include:

-   `analysis to perform`: 1
-   `name of construction`: QILAI
-   `corpus size`: `r sum(apple_word_freq$n)`
-   `Fisher-Yates`: no 
-   `sorting`: 4 (=collostruction strength)
-   `tab-delimited input data`: \<qilai.tsv\>

------------------------------------------------------------------------

If everything works properly, you should get the output of `coll.analysis.r` as a text file `qilai_results.txt` in your working directory.

The text output from Gries' script may look as follows.

```{r echo=F, purl=F, out.width="100%"}
knitr::include_graphics("images/coll-analysis-result.png")
```

## Interpretations

The output from `coll.analysis.r` is a text file with both the **result data frame** (i.e., the data frame with all the statistics) as well as detailed annotations/explanations provided by Stefan Gries.

We can extract the result data frame from the text file. A sample output file from the collexeme analysis of QILAI has been made available in `demo_data/qilai_results.txt`.

To extract the result data frame from the script output:

1.  We first load the result txt file like a normal text file using `readlines()`
2.  We extract the lines which include the statistics and parse them as a delimited table (i.e., TSV) into a data frame using `read_tsv()`

```{r purl=T}

## convert into CSV
collo_table<-read_tsv("2025-10-01 09-43-32.59546.csv")

## auto-print
collo_table %>%
  filter(relation =="attraction") %>%
  arrange(desc(coll.strength)) %>%
  head(100) %>%
  select(words, coll.strength, everything())
```

The most important column is `coll.strength`, which is a statistical measure indicating the **association strength** between each collexeme and the construction. Please do check @stefanowitsch2003 very carefully on how to interpret these numbers.

------------------------------------------------------------------------

With the collexeme analysis statistics, we can therefore explore the top N collexemes according to specific association metrics.

Here we look at the top 10 collexemes according to four different distributional metrics:

1.  `obs.freq`: the raw joint frequency of the word and construction.
2.  `delta.p.constr.to.word`: the delta P of the construction to the word
3.  `delta.p.word.to.constr`: the delta P of the word to the construction
4.  `coll.strength`: the log-transformed *p*-value based on Fisher exact test

```{r out.height = "100%", purl=T}
## from wide to long
collo_table %>%
  filter(relation == "attraction") %>%
  filter(obs.freq >=5) %>%
  select(words, obs.freq, 
         delta.p.constr.to.word, 
         delta.p.word.to.constr,
         coll.strength) %>%
  pivot_longer(cols=c("obs.freq", 
                      "delta.p.constr.to.word", 
                      "delta.p.word.to.constr",
                      "coll.strength"),
               names_to = "metric",
               values_to = "strength") %>%
  mutate(metric = factor(metric, 
                         levels = c("obs.freq", 
                                   "delta.p.constr.to.word",
                                   "delta.p.word.to.constr",
                                   "coll.strength"))) %>%
  group_by(metric) %>%
  top_n(10, strength) %>%
  ungroup %>%
  arrange(metric, desc(strength)) -> coll_table_long

## plot
coll_table_long %>%
    mutate(words = reorder_within(words, strength, metric)) %>%
    ggplot(aes(words, strength, fill=metric)) +
    geom_col(show.legend = F) +
    coord_flip() +
    facet_wrap(~metric,scales = "free") +
    tidytext::scale_x_reordered() + 
    labs(x = "Collexemes",
         y = "Strength",
         title = "Collexemes Ranked by Different Metrics")
```

```{r eval=F, echo=F, purl=F}
## This is the old version to create sorted bar plots
# graphs <- list()
# for(i in levels(coll_table_long$metric)){
#   coll_table_long %>%
#     filter(metric %in% i) %>%
#     ggplot(aes(reorder(words, strength), strength, fill=strength)) +
#     geom_col(show.legend = F) +
#     coord_flip() +
#     labs(x = "Collexemes", 
#          y = "Strength", 
#          title = i)+
#     theme(text = element_text(family="Arial Unicode MS"))-> graphs[[i]]
# }
# 
# require(ggpubr)
# ggpubr::ggarrange(plotlist = graphs)
```

The bar plots above show the top 10 collexemes based on four different metrics: `obs.freq`, `delta.p.contr.to.word`, `delta.p.word.to.contr`, and `coll.strength`.

::: alert
Please refer to the assigned readings on how to compute the **collostrengths**. Also, in @stefanowitsch2003, please pay special attention to the parts where Stefanowitsch and Gries are arguing for the advantages of the collostrengths based on the Fisher Exact tests over the traditional raw frequency counts.

Specifically, **delta P** is a very unique association measure. It has received increasing attention in psycholinguistic studies. Please see @ellis2006 and @gries2013 for more comprehensive discussions on the issues of association's directionality. I need everyone to have a full understanding of how **delta p** is computed and how we can interpret this association metric.
:::

<!-- *** -->

<!-- ```{block, type="info", purl=F} -->

<!-- Many studies have shown that Chinese makes use of large proportion of four-character idioms in the discourse. Therefore, four-character idioms are often one of the hot topics in the study of constructions in Chinese. -->

<!-- In our `demo_data` directory, there is a file `dict-ch-idiom.txt`, which includes a list of four-character idioms in Chinese. These idioms are collected from [搜狗輸入法詞庫](https://pinyin.sogou.com/dict/) and the original file formats (`.scel`) have been combined, removed of duplicate cases, and converted to a more machine-readable format, i.e., `.txt`. -->

<!-- You can load the dataset in R for more exploration of idioms. -->

<!-- ``` -->

<!-- ```{r eval=F, echo=T, purl=T} -->

<!-- all_idioms <- readLines(con = "demo_data/dict-ch-idiom.txt") -->

<!-- head(all_idioms) -->

<!-- tail(all_idioms) -->

<!-- length(all_idioms) -->

<!-- ``` -->

::: exercise
If we look at the top 10 collexemes ranked by the collostrength, we would see a few puzzling collexemes, such as `一`, `了`, `不`. Please identify these puzzling construction tokens as concordance lines (using `quanteda::kwic()`)and discuss their issues and potential solutions.
:::

```{r eval=T, echo=F}
apple_tokens<-as.tokens(str_split(apple_df$text_tag,"\\s+"))
kwic(apple_tokens, pattern = phrase("了 起來"))
kwic(apple_tokens, pattern = phrase("一 起來"))
kwic(apple_tokens, pattern = phrase("不 起來"))
```

------------------------------------------------------------------------

## Exercises

The following exercises should use the dataset [Yet Another Chinese News Dataset](https://www.kaggle.com/ceshine/yet-another-chinese-news-dataset) from Kaggle.

The dataset is available on our dropbox `demo_data/corpus-news-collection.csv`.

The dataset is a collection of news articles in Traditional and Simplified Chinese, including some Internet news outlets that are NOT Chinese state media.

```{r eval=T, echo=F, purl=F}
news_demo <-read_csv("demo_data/corpus-news-collection.csv",n_max = 20)
news_demo %>%
  mutate(desc = str_sub(desc, 0,50))
```

------------------------------------------------------------------------

```{exercise}
Please conduct a collexeme analysis for the aspectual construction "X + 了" in Chinese. 

Extract all tokens of this consturction from the news corpus and identify all words preceding the aspectual marker.

Based on the distributional information, conduct the collexemes analysis using the `coll.analysis.r` and present the collexemes that significantly co-occur with the construction "X + 了" in the X slot. Rank the collexemes according to the collostrength provided by Stefan Gries' script.
```

::: danger
When you tokenize the texts using `jiebaR`, you may run into an error message as shown below. If you do, please figure out what may have contributed to the issue and solve the problem on your own.

```{r eval=T, echo=F, purl=F, out.width="100%"}
knitr::include_graphics("images/error-jiebaR.png")
```
:::

```{r eval=T, echo=F, purl=F}
library(readr)
library(tidyverse)
library(quanteda)
library(readtext)
library(tidytext)
library(jiebaR)
library(tmcn)

# load corpus
news_corpus <-readtext("demo_data/corpus-news-collection.csv", 
                       text_field = "desc",
                       encoding = "UTF-8") %>% corpus

# con <- file("demo_data/corpus-news-collection.csv", encoding="UTF-8")
# news_corpus_2 <- read.csv(file=con)
# 
# news_corpus_2 %>%
#   filter(str_detect(desc,"^\\.+$")) %>% nrow
# news_corpus %>% tidy %>%
#   filter(str_detect(text,"^\\.+$")) %>% nrow

# convert tidy (size: 136339)
news_tidy <- news_corpus %>% 
  tidy %>% 
  filter(!str_detect(text, "^\\.+$")) %>% ## weird docs
  filter(!str_detect(text, "^\\s*$")) %>% ## empty docs
  mutate(text = toTrad(text)) %>% ## sim2traditional
  #mutate(text = str_replace_all(text, "\\s+"," ")) %>% ## cleaning
  mutate(text = normalize_document(text)) %>%
  mutate(text_id = row_number()) 



# Initialize the segmenter
segmenter <- worker(user="demo_data/dict-ch-user.txt", 
                    bylines = T, 
                    symbol = T)

# word_seg_text <- function(text, tagger){
#     segment(text, jiebar = tagger) %>%
#     str_c(collapse=" ")
# }

news_tidy %>%
  unnest_tokens(word,
                text,
                token = function(x) segment(x, segmenter)) %>%
  filter(nzchar(word) & !str_detect(word, "^(\\s|\u3000)+$"))-> news_tidy_word # 8131049

# x<-map(news_tidy$text, function(x) word_seg_text(text = x, tagger = segmenter))

# debug
# x <- vector(length=length(news_tidy$text))
# for(i in 1:length(news_tidy$text)){
#   cat(i, " ")
#   x[i] <-word_seg_text(news_tidy$text[i], tagger = segmenter)
# }

# system.time(news_tidy2 <- news_tidy %>% # create doccument index
#   mutate(text_tag = map_chr(text, word_seg_text, segmenter)))


news_tidy_word %>%
  group_by(text_id) %>%
  summarize(text_tag = str_c(word, collapse=" ")) %>%
  ungroup -> news_text_tag

news_tidy %>%
  left_join(news_text_tag, by = c("text_id" = "text_id")) -> news_tidy2


news_tidy2 %>%
  unnest_tokens(pattern, 
                text_tag,
                token = function(x) str_extract_all(x, "\\s[^\\s]+\\s+\\了\\s")) %>%
  filter(!is.na(pattern))-> kwic_le

news_word_freq <- news_tidy_word %>%
  count(word, sort=T)

#kwic_le[c(3656, 4090, 11550, 11617), ] %>% View

kwic_le %>%
  select(text_id, pattern) %>% 
  mutate(pattern = str_trim(pattern)) %>%
  tidyr::separate(col="pattern",
                  into=c("verb","le"),
                  sep="\\s+") %>%
  count(verb, sort=T) %>%
  mutate(verb_freq = news_word_freq$n[match(verb, news_word_freq$word)]) %>%
  select(verb, verb_freq, n) -> collo_input

collo_input %>%
  #filter(n >= 20) %>% 
  write_tsv("le.tsv")
corpus_size <- sum(news_word_freq$n)
construction_size<-sum(collo_input$n)
```

-   It is suggested that you parse/tokenize the corpus data and create two additional columns to the text-based data frame --- `text_id`, and `text_tag`. The following is an example of the first ten articles.

```{r eval=T, echo=F, purl=F}
news_tidy2 %>% head(10) %>%
  select(text_id, text, text_tag, everything())
```

-   A word frequency list of the top 100 words is attached below (word tokens that are pure whitespaces or empty strings were not considered)

```{r eval=T, echo=F, purl=F}
news_word_freq %>% head(100)
```

-   After my data preprocessing and tokenization, here is relevant distributional information for `coll.analysis.r`:

    -   Corpus Size: `r corpus_size`
    -   Consturction Size: `r construction_size`

```{r eval=F, echo=F, purl=F}
# - `analysis to perform`: 1
# - `name of construction`: LE
# - `corpus size`: 7894153
# - `freq of constructions`: 25618
# - `index of association strength`: 1 (=fisher-exact)
# - `sorting`: 4 (=collostruction strength)
# - `decimals`: 8
# - `text file with the raw data`: <qilai.tsv>
# - `Where to save output`: 1 (= text file)
# - `output file`: <qilai_results.txt>

file.create("le_results.txt")
source("http://www.stgries.info/teaching/groningen/coll.analysis.r")
#source("../../../R/coll.analysis_mpfr-alvin.r") # Inf issues
```

-   The output of the Collexeme Analysis (`coll.analysis.r`)

```{r eval=T, echo=F, purl=F}
results <-readLines("le_results.txt")
results<-results[-c(1:17, (length(results)-17):length(results))]
collo_table<-read_tsv(I(results))
collo_table %>% 
  filter(relation=="attraction") %>%
  top_n(100, coll.strength) %>%
  arrange(desc(coll.strength)) %>%
  select(words, word.freq, obs.freq, exp.freq, relation, coll.strength, everything())
```

-   When plotting the results, if you have `Inf` values in the `coll.strength` column, please replace all the `Inf`values with the maximum numeric value of the `coll.strength` column.

```{r eval=T, echo=F, purl=F}
# deal with Inf
collo_table$coll.strength[which(collo_table$coll.strength==Inf)]<-max(collo_table$coll.strength[which(collo_table$coll.strength!=Inf)]) 

# from wide to long
collo_table %>%
  filter(relation == "attraction") %>%
  filter(obs.freq >= 10) %>%
  select(words, obs.freq, 
         delta.p.constr.to.word, 
         delta.p.word.to.constr,
         coll.strength) %>%
  pivot_longer(cols=c("obs.freq", 
                      "delta.p.constr.to.word", 
                      "delta.p.word.to.constr",
                      "coll.strength"),
               names_to = "metric",
               values_to = "strength") %>%
  mutate(metric = factor(metric, 
                         levels = c("obs.freq", 
                                   "delta.p.constr.to.word",
                                   "delta.p.word.to.constr",
                                   "coll.strength"))) %>%
  group_by(metric) %>%
  top_n(10, strength) %>%
  #arrange(strength) %>%
  #mutate(strength_rank = row_number()) %>%
  ungroup %>%
  arrange(metric, desc(strength)) -> coll_table_long

# plot
# 

coll_table_long %>%
    mutate(words = reorder_within(words, strength, metric)) %>%
    ggplot(aes(words, strength, fill=metric)) +
    geom_col(show.legend = F) +
    coord_flip() +
    facet_wrap(~metric,scales = "free") +
    tidytext::scale_x_reordered() + 
    labs(x = "Collexemes",
         y = "Strength",
         title = "Collexemes Ranked by Different Metrics")

# graphs <- list()
# for(i in levels(coll_table_long$metric)){
#   coll_table_long %>%
#     filter(metric %in% i) %>%
#     #mutate(strength = jitter(strength)) %>%
#     ggplot(aes(reorder(words, strength), strength, fill=strength)) +
#     geom_col(show.legend = F) +
#     coord_flip() +
#     labs(x = "Collexemes", 
#          y = "Strength", 
#          title = i)+
#     theme(text = element_text(family="Arial Unicode MS"))+
#     scale_fill_gradient()-> graphs[[i]]
# }
# 
# require(ggpubr)
# ggpubr::ggarrange(plotlist = graphs)
```

------------------------------------------------------------------------

```{exercise}
Using the same Chinese news corpus---`demo_data/corpus-news-collection.csv`, please create a frequency list of all four-character words/idioms that are included in the four-character idiom dictionary `demo_data/dict-ch-idiom.txt`. 

Please include both the frequency as well as the dispersion of each four-character idiom in the corpus. Dispersion is defined as the number of articles where it is observed.

Please arrange the four-character idioms according to their dispersion.
```

```{r eval=T, echo=F, purl=F}
library(readr)
library(tidyverse)
library(readtext)
news <-readtext("demo_data/corpus-news-collection.csv",
                text_field = "desc",
                encoding = "UTF-8") %>% corpus %>% tidy #%>% select(title, desc, source, date)
# clean up
news %>% 
  filter(!str_detect(text, "^\\.+$")) %>%
  mutate(text = toTrad(text)) %>%
  #mutate(text = str_replace_all(text, "\\s+"," ")) %>%
  mutate(text = normalize_document(text)) %>%
  mutate(date = lubridate::ymd(date),
         text_id = row_number()) ->news
# intialize worker
library(jiebaR)
library(tidytext)
segmenter_word <- worker(user="demo_data/dict-ch-user.txt", 
                    bylines = T, 
                    symbol = T)
system.time(news %>% 
  unnest_tokens(word,
                text, 
                token = function(x) segment(x, segmenter_word))-> news_word)

#compare with previous exercise
# news_tidy2 %>%
#   unnest_tokens(word,
#                 text_tag,
#                 token = function(x) str_split(x, pattern = " ")) %>%
#   filter(nzchar(word))->news_word2
# 
# nrow(news_word2)
# data.frame(head(news_word2$word, 100), head(news_word$word,100))
# corp size
# nrow(news_word)

# four-char words
all_idioms <- readLines(con = "demo_data/dict-ch-idiom.txt", encoding = "UTF-8")


# idiom types
news_word %>%
  filter(word %in% all_idioms) -> news_idiom

news_idiom %>%
  group_by(word) %>%
  summarize(freq = n(), dispersion = n_distinct(text_id)) %>%
  arrange(desc(dispersion))
```

```{exercise}
Let's assume that we are particularly interested in the idioms of the schema of `X_X_`, such as "一心一意", "民脂民膏", "滿坑滿谷" (i.e., idioms where the first character is the same as the third character).

Please find the top 20 frequent idioms of this schema and visualize their frequencies in a bar plot as shown below.
```

```{r ch9graphnews1, eval=T, echo=F, purl=F}
news_idiom %>%
  filter(str_detect(word, "(.).\\1.")) %>%
  mutate(idiom = word) %>%
  tidyr::extract(col = "word",
                 into = c("pivot","var1", "var2"),
                 regex = "(.)(.).(.)") %>%
  mutate(schema = paste(pivot,"_",pivot,"_", sep="")) -> news_idiom2
# idiom frequency
news_idiom2 %>%
 count(idiom, sort = T) %>%
  top_n(20, n) %>%
  ggplot(aes(reorder(idiom, n),n, fill=n)) + 
  geom_col(show.legend = F) +
  theme(text = element_text(family="Arial Unicode MS")) +
  coord_flip() +
  labs(x = "Four-Character Words", y = "Frequency") +
  scale_fill_gradient2(trans = "log")
```

```{exercise}
Continuing the previous exercise, use the same set of idioms of the schema `X_X_` and identify the `X`. Here we refer to the character `X` as the pivot of the idiom. 

Please identify all the pivots for idioms of this schema which have at least two types of constructional variants in the corpus (i.e., its **type frequency** >= 2) and visualize their type frequencies as shown below.
```

```{r ch9graphnews2, eval=T, echo=F, purl=F}
# Top 5 of each Schema
news_idiom2 %>%
  group_by(schema) %>%
  summarize(token = n(), type = n_distinct(idiom)) %>%
  filter(type >=2) %>%
  arrange(desc(type)) -> schema_freq

schema_freq

schema_freq %>%
  ggplot(aes(reorder(schema, type), type, fill=type)) + 
           geom_col(show.legend = F) +
           theme(text = element_text(family="Arial Unicode MS")) +
           coord_flip() +
           labs(x = "Four-Character Words", y = "Frequency") +
           scale_fill_gradient2(trans = "log")
```

-   For example, the **type frequency** of the most productive pivot schema, "不_不\_", is `r schema_freq$type[1]` in the news corpus. That is, there are `r schema_freq$type[1]` types of constructional variants of this schema with the pivot `不`, as shown below:

```{r eval=T, echo=F, purl=F}
news_idiom2 %>%
  filter(schema %in% schema_freq$schema[1]) %>%
  select(idiom) %>%
  count(idiom, sort=T) %>%
  rename(freq = n)
```

```{exercise}
Continuing the previous exercise, to further study the semantic uniqueness of each pivot schema, please identify the top 5 idioms of each pivot schema according to the frequencies of the idioms in the corpus. 

Please present the results for schemas whose **type frequencies** >= 5 (i.e., the pivot schema has at least FIVE different idioms as its constructional instances).

Please visualize your results as shown below.
```

```{r ch9graphnews3, eval=T, echo=F, purl=F}
news_idiom2 %>%
  group_by(schema) %>%
  summarize(token = n(), type = n_distinct(idiom)) %>%
  filter(type >=5) %>%
  arrange(desc(type)) -> template_w_5_types

news_idiom2 %>%
  filter(schema  %in% template_w_5_types$schema) %>%
  mutate(schema = factor(schema, levels = template_w_5_types$schema)) %>%
  group_by(schema) %>%
  count(idiom, sort=T) %>%
  top_n(5, n) %>%
  ungroup() %>%
  arrange(schema, desc(n)) -> template_w_5_types_examples 

template_w_5_types_examples %>%
  ggplot(aes(reorder(idiom, n), n, fill=schema)) + 
           geom_col(show.legend = F) +
           theme(text = element_text(family="Arial Unicode MS")) +
           coord_flip() +
           labs(x = "Four-Character Words", y = "Frequency") +
           #scale_fill_gradient2(trans = "log") +
  facet_wrap(~schema, scale="free")
      
```

```{exercise}
Let's assume that we are interested in how different media may use the four-character words differently.

Please show the average number of idioms per article by different media and visualize the results in bar plots as shown below.

The average number of idioms per article for each media source can be computed based on **token** frequency (i.e., on average how many idioms were observed in each article?) or **type** frequency (i.e., on average how many different idiom types were observed in each article?).
```

```{r eval=T, echo=F, purl=F}
# how each media uses four-char words
news_word %>%
  filter(word %in% all_idioms) %>%
  group_by(source) %>%
  summarize(token = n(), type = n_distinct(word), N = n_distinct(text_id)) %>%
  mutate(token_mean = token/N, 
         type_mean = type/N) %>%
   arrange(desc(token_mean))-> source_idiom_use
```

-   For example, there are `r source_idiom_use$token[1]` tokens (`r source_idiom_use$type[1]` types) of idioms observed in the `r source_idiom_use$N[1]` articles published by "`r source_idiom_use$source[1]`".

    -   The average **token** frequency of idiom uses would be: `r source_idiom_use$token[1]`/`r source_idiom_use$N[1]` = `r round(source_idiom_use$token_mean[1],2)`;
    -   the average **type** frequency of idiom uses would be: `r source_idiom_use$type[1]`/`r source_idiom_use$N[1]` = `r round(source_idiom_use$type_mean[1],2)`.

```{r ch9graphnews4, eval=T, echo=F, purl=F}
source_idiom_use 

source_idiom_use %>%
  mutate(token = token/N, type = type/N) %>%
  pivot_longer(cols = c("token", "type"),
               names_to = "frequency") %>%
  filter(frequency == "token") %>%
  ggplot(aes(x = reorder(source, value), y = value, fill=frequency)) +
  geom_col(show.legend=F, fill = "lightpink") +
  theme(text = element_text(family="Arial Unicode MS")) +
  #facet_wrap(~frequency, scale="free_y") +
  labs(x="",y="", title = "Token Frequency") +
  coord_flip() ->g1
  
source_idiom_use %>%
  mutate(token = token/N, type = type/N) %>%
  pivot_longer(cols = c("token", "type"),
               names_to = "frequency") %>%
  filter(frequency == "type") %>%
  ggplot(aes(x = reorder(source, value), y = value, fill=frequency)) +
  geom_col(show.legend=F, fill="lightblue") +
  theme(text = element_text(family="Arial Unicode MS")) +
  #facet_wrap(~frequency, scale="free_y") +
  labs(x="",y="", title="Type Frequency")+
  coord_flip() ->g2

ggpubr::ggarrange(g1,g2)

```
